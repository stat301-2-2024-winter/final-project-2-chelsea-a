---
title: "Predicting Students' College Enrollment Outcomes"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: Chelsea Angwenyi
date: today

format:
  html:
    toc: true
    toc-depth: 4
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false
  
from: markdown+emoji  
---

::: {.callout-tip icon="false"}
## Github Repo Link

[My Github Repo URL](https://github.com/stat301-2-2024-winter/final-project-2-chelsea-a/tree/main)
:::

```{r}
#| label: load-packages
#| echo: false
library(tidyverse)
library(tidymodels) 
library(here)
```

## Introduction

This project looks at students enrolled in a Portugal college and uses their demographics, academic path, and more to predict their enrollment outcomes. This is a multiclass classification task and predicts whether an undergraduate student drops out, graduates, or remains enrolled in the institution.

The UC Irvine Machine Learning Repository is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The dataset was created in a project that aims to contribute to the reduction of academic dropout and failure in higher education, by using machine learning techniques to identify students at risk at an early stage of their academic path, so that strategies to support them can be put into place. Therefore a predictive model would be used for reasons that would be able to find factors that put students at a higher chance of dropping out and provide resources to them so that they can achieve their degree.

The data was acquired from several disjoint databases and holds over four thousand observations of student outcomes at this school with 36 predictor variables. It uses variables related to semester credits, grades, and area of study, as well as individual factors such as marital status and whether they had daytime or evening classes. I wanted to learn more about what some factors were that make some students more likely to drop out than others.

I chose this dataset because I have an interest in social policy and have learned a lot about education policy in my studies. I have also been able to learn specifically about colleges and saw this dataset as an opportunity to learn more about education and student outcomes in college. Therefore, some initial things that I wanted to explore are all of the predictors that impact enrollment outcomes, if at all, and which factors made the best predictor.

## Data Overview

Upon initial exploration of the target variable on the entire dataset, it becomes evident that there is a severe class imbalance. There is a total of 4,424 observations and out of these about 20% remain enrolled, 32% dropped out, and a whopping 50% graduate in this dataset. Additionally, data was published completely clean with zero missingness on any of the variables. See @fig-name-of-figure

![Distribution of Student Enrollment Outcomes](figures/count_plot.png){#fig-name-of-figure}

## Methods

For my classification model, I allocated 80% of the data to the training set, ensuring that a substantial portion of the data is allocated for training our models. The remaining 20% was allocated to the testing set and left unseen. Since there is a significant imbalance amongst the classes in the target variable, I stratified by the target variable to ensure that my data subsets are balanced. Considering that my prediction model is for multiclass classification problems, I chose compatible model types for my data. These are the random forest, boosted tree, multinomial, elastic net, and K-nearest neighbor model. Additionally, I used a naive Bayes model for my baseline model along with a null model.

For my data, I chose to employ cross-validation folds (student_folds) with 10 folds and 5 repeats. By dividing the data into multiple folds and repeating the process several times, we ensure that each observation in the dataset is used for both training and testing at least once. This approach helps mitigate the risk of overfitting and provides a more accurate assessment of the model's performance. Like the initial split, I stratified the cross-validation folds based on the target variable. This ensures that each fold contains a representative sample of the different classes within the target variable, enhancing the reliability of our model evaluation.

I made three models for my linear model recipe, one as a base model and two as models that I would go forward with. I made the base model with only one operation to assess the performance of my following recipes and note whether the added operations are making it better or worse. I have two tree-based recipes, and lastly, I have a recipe for my naive Bayes model. Ultimately, I will be using the accuracy metric to compare and assess each model. This metric measures the percentage of correct predictions. For all models, I used parallel processing to speed up computation.

## Model Building & Selection

I will be using the accuracy metric to assess all of the models. To start, the null and the naive Bayes models performed the worst, as expected, considering they are baseline models. From there, my linear models, KNN, and tree-based models performed better. I will evaluate each of these.


```{r}
#| echo: FALSE
#| label: acc-tbl
read_csv(here("figures/acc_table.csv")) |> 
  knitr::kable(digits = c(NA, 3, 4),
               caption = "Model Accuracy Metrics")
```

### Linear Models
The multinomial and elastic net models, both performed best from my most complex linear recipe. I used a correlation plot to detect hidden patterns among variables that could affect the target variable. In @fig-cor-plot, I found that there was a very strong correlation between academic factors such as semester units credited, units evaluated, units enrolled, units approved, and semester grades. There was also a perfect correlation between mothers' and fathers' occupations. This led me to add interactions between everything that had a perfect correlation to my recipe. I also added an interaction between the variables mothers' qualification and fathers' qualification, as these identified parents' highest level of education which  I believed to be relevant for predicting student outcomes. Lastly, I included an interaction between the semesters without evaluations because I also believed this could be a predictor specifically for dropout.



![Correlation Plot](figures/correlation_plot3.png){#fig-cor-plot}

For the multinomial model specifications, I added a penalty parameter of 0 to avoid regularization and fit the model to the folds. For the elastic net model specifications, I tuned both the penalty and the mixture. For hyperparameter tuning, I created a grid of tuning values and specified five levels for each hyperparameter. Overall, the multinomial model and elastic net model had the same accuracy of .769 with the Elastic Net having a slightly lower standard of error. Therefore, these models predicted around 77% of the outcomes correctly in the multiclass data.

### K-nearest neighbor Model
For the KNN model, I could use either my tree-based or linear model recipes. From what research showed and my model performance, I decided to fit the KNN model on my tree-based recipe. It performed best with my second recipe which centered and scaled all numeric predictors, dummied categorical variables, and removed highly sparse variables. Beyond this, it removed several predictor variables. The recipe removed GDP, inflation rate, unemployment rate, fathers qualification, mothers qualification, fathers occupation, and mothers occupation. I removed the predictors on parents' qualifications and occupations because I believed that the high number of levels could make them ineffective predictors. I also removed GDP, inflation rate, and unemployment rate because considering the observations were all individuals, these seemed irrelevant to the task. This is the recipe the KNN model performed best with.

Moving to model specifications, I specified key hyperparameters such as the number of neighbors, the weight function, and the distance power. The tuning grid for this model also uses three levels. The resulting accuracy was .734 or about 73% in predictive accuracy.

### Tree-based Models
The tree-based models, random forest, and boosted tree both performed best from my basic tree recipe. I used a recipe that centered and scaled all numeric predictors, dummied all categorical variables, and removed variables that were highly sparse and unbalanced. This is the recipe that my tree-based models performed the best with which I found interesting because the KNN performed worse with this recipe. For the random forest model, adjustments showed that 1000 trees led to the best performance. I tuned the mtry and the min_n, specifically updating the range of mtry parameter to vary between 6 and 18. Further, I chose 5 levels for optimal results in my hyperparameters.

In the boosted tree defining key hyperparameters such as the number of trees, learning rate, the number of variables randomly sampled at each split, and the minimum number of data points in a node. Specifically, I set the range for the number of trees to vary between 100 and 750, mtry to range from 3 to 18, and the learning rate to range between 0.01 and 0.3. I went with three levels to balance performance with efficiency with this model.

Overall, the boosted tree performed better than the random forest at .779 and .775 respectively. With the accuracy of the boosted tree predicting 78% of the outcomes correctly it performs better than the random forest and the linear models.

### The best model

In summary, the boosted tree performed best in comparison to every model with an accuracy of 78%. This is not surprising considering that the boosted tree model is a complex model that creates a series of decision trees to produce a final prediction. These models also took the longest to run which, fortunately, corresponded to better predictions.

## Final Model Analysis
Since the boosted tree performed the best based on accuracy this is the model I chose as my winning model. After fitting it to the entire training set, the final model had an accuracy of 78.4% when applied to testing data. In other words, the final boosted tree model predicted the highest proportion of instances correctly. In comparison, my naive Bayes baseline model's accuracy was at 58.6% which puts my final model above my baseline by a substantial 20% which is generally positive.

I will also use a confusion matrix as a tool for assessing model performance across different classes. A confusion matrix is a table that shows the counts of true positive, true negative, false positive, and false negative predictions for each class. From the confusion matrix, we see that 220 students were correctly predicted as 'Dropout', while 36 students were incorrectly predicted as 'Enrolled' when they were actually a 'Dropout'. Additionally, the model incorrectly predicted 29 students to be a 'Graduate' when they were actually a 'Dropout'. Furthermore, the model correctly predicted 64 students as 'Enrolled', while incorrectly classifying 27 students as a 'Dropout' when they were actually 'Enrolled'. Lastly, the model correctly predicted 411 students as 'Graduate' and incorrectly classified 68 students as 'Enrolled' instead of 'Graduate'. 

```{r}
#| label: conf-matrix
#| echo: false

read_rds(here("figures/conf_mat.rds")) |> knitr::kable(caption = "Confusion Matrix")

```

I then visualized the class-wise accuracy of the model using a bar graph. In @fig-acc-of-model, each bar represents a different class, indicating the accuracy of the model in predicting that particular class. The model's accuracy for predicting a graduate was very high at 93%. It also did a good job of predicting whether a student dropped out at 77.2% but a bad job of predicting whether a student was to remain enrolled at 40.3%.

![Class-wise Accuracy of Model](figures/accuracy_plot.png){#fig-acc-of-model}

I will also evaluate the model using the area under the ROC curve, the ROC AUC. This metric calibrates the trade-off between sensitivity and specificity at the best-chosen threshold and is regarded as an important measure of how good an algorithm is. The ROC AUC value ranges from 0 to 1, where 0 represents a model that makes all predictions incorrectly, and 1 represents a perfect model that makes all predictions correctly. The ROC AUC for the final model was .890 whereas for my baseline it was .787 which indicates that the final model's ability to distinguish between positive and negative classes is high.

Next, I plotted ROC curves for each class to give insights into the model's ability to distinguish between different classes. The ROC curve in @fig-student-roc-curve also shows that the model is very well at predicting graduates, good at predicting dropouts, and not so well at predicting students who remained enrolled. 

![ROC curve](figures/student_curve.png){#fig-student-roc-curve}
 In summary, there are some features of the model that made it the best. Since I used a boosted tree model, it possesses the flexibility to capture nonlinearity in the model well. It used 750 trees and decision trees are inherently flexible and can model intricate interactions between features. With this model, I did not remove any predictors from the recipe, indicating the model found interactions between those predictors. I also believe that the low learn rate of .01 improved the model's generalization performance.
 
## Conclusion
Finally, I looked at the importance of features in predicting class outcomes in my model. From @fig-variable-plot, it becomes evident that approved units for both semesters but especially the second semester were the most important predictors in the model. The third most important predictor was second semester grades, which would align with what I would assume. Other important predictors were tuition and fees that were not up to date, the course route of the student, first semester grades, and age at enrollment. This I find interesting because I am curious to know if younger enrollment leads to higher dropout rates or vice versa. 

![Variable Importance Plot](figures/important.png){#fig-variable-plot}

An interesting variable that was also an important predictor was the mother's occupation. Initially, I believed that this variable would not be very useful because of how many occupations there were, however, it proved to be important in the boosted tree model. 

While this data is specifically about a higher education institution in Portugal, valuable findings can be taken from it and applied to institutions around the world. Firstly, there is a very strong correlation between variables concerning academic outcomes and units. Additionally, there is a very high importance of academic outcomes in predicting student graduation and dropout rates. The most important predictor was the number of curricula units approved in the second semester. This school should consider this for policies geared toward increasing graduation rates for their students. 

## References

M.V.Martins, D. Tolledo, J. Machado, L. M.T. Baptista, V.Realinho. (2021) "Early prediction of student’s performance in higher education: a case study" Trends and Applications in Information Systems and Technologies, vol.1, in Advances in Intelligent Systems and Computing series. Springer. DOI: 10.1007/978-3-030-72657-7_16 https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success

This dataset is supported by the program SATDAP - Capacitação da Administração Pública under grant POCI-05-5762-FSE-000191, Portugal.
